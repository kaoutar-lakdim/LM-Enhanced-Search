# Integrating Pre-Trained Language Models and Embeddings for Enhanced Search and Retrieval

<h1 align="center">
    <img src="https://readme-typing-svg.herokuapp.com/?font=Righteous&size=35&center=true&vCenter=true&width=500&height=70&duration=4000&lines=Hi+There!+ðŸ‘‹;+Take+a+look+into+our+project!;" />
</h1>

# Contributors


<table>
  <tr>
    <td width="50%">
      <h3>Yassine Squalli Houssaini</h3>
    </td>
    <td width="50%">
      <a href="mailto:squayassine@gmail.com">
        <img src="https://img.shields.io/badge/Gmail-333333?style=for-the-badge&logo=gmail&logoColor=red" />
      </a>
      <a href="https://www.linkedin.com/in/yassine-squalli-houssaini-2abb1a255/" target="_blank">
        <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank" />
      </a>
      <a href="https://salesp07.github.io" target="_blank">
         <img src="https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=todoist&logoColor=white" target="_blank" />
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <h3>Kaoutar Lakdim</h3>
    </td>
    <td>
      <a href="mailto:pedro.sales.muniz@gmail.com">
        <img src="https://img.shields.io/badge/Gmail-333333?style=for-the-badge&logo=gmail&logoColor=red" />
      </a>
      <a href="https://linkedin.com/in/pedro-sales-muniz" target="_blank">
        <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank" />
      </a>
      <a href="https://salesp07.github.io" target="_blank">
         <img src="https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=todoist&logoColor=white" target="_blank" />
      </a>
    </td>
  </tr>
</table>


<h3 align="left">Languages and Tools:</h3>
<p align="left"> <a href="https://aws.amazon.com" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/aec684f0-f726-46d8-89ad-e596dbb91619" alt="aws" width="300" height="200"/> </a> <a href="https://azure.microsoft.com/en-in/" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/940d904a-3884-4ca7-b5c7-1ac01bded5cc" alt="azure" width="300" height="200"/> 











---

### Table of Contents
- [Integrating Pre-Trained Language Models and Embeddings for Enhanced Search and Retrieval](#integrating-pre-trained-language-models-and-embeddings-for-enhanced-search-and-retrieval)
- [Contributors](#contributors)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [License](#license)


---
# Media Retrieval and Captioning with CLIP

This Jupyter Notebook demonstrates how to use OpenAI's CLIP model for media retrieval and captioning. Given an input query, the code retrieves the best matching image or video frame from a provided dataset. The CLIP model is utilized to encode both text queries and media embeddings, enabling efficient similarity calculations.

## Usage

1. **Run Each Cell**: Execute each code cell in sequential order to set up the environment and define the functions.

2. **Upload Media**: Run the cells that define the Gradio interface to upload an image or video file.

3. **Enter Query**: Provide a query related to the content you're looking for.

4. **Select Mode**: Choose between "image" or "video" mode for retrieval.

5. **Provide Image Folder Path**: If using image mode, run the cells that define the image retrieval function and provide the path to the folder containing images.

6. **Results**: The interface will display the best matching media along with a caption.

## Example

<p align="center">
  <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/74473164/50a89f82-4da1-43bc-8789-4a3bd0564f33" alt="schema">
</p>


---


