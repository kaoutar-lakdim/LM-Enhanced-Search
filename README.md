# Integrating Pre-Trained Language Models and Embeddings for Enhanced Search and Retrieval

<h1 align="center">
    <img src="https://readme-typing-svg.herokuapp.com/?font=Righteous&size=35&center=true&vCenter=true&width=500&height=70&duration=4000&lines=Hi+There!+ðŸ‘‹;+Take+a+look+into+our+project!;" />
</h1>

# Contributors


<table>
  <tr>
    <td width="50%">
      <h3>Yassine Squalli Houssaini</h3>
    </td>
    <td width="50%">
      <a href="mailto:squayassine@gmail.com">
        <img src="https://img.shields.io/badge/Gmail-333333?style=for-the-badge&logo=gmail&logoColor=red" />
      </a>
      <a href="https://www.linkedin.com/in/yassine-squalli-houssaini-2abb1a255/" target="_blank">
        <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank" />
      </a>
      <a href="https://salesp07.github.io" target="_blank">
         <img src="https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=todoist&logoColor=white" target="_blank" />
      </a>
    </td>
  </tr>
  <tr>
    <td>
      <h3>Kaoutar Lakdim</h3>
    </td>
    <td>
      <a href="mailto:pedro.sales.muniz@gmail.com">
        <img src="https://img.shields.io/badge/Gmail-333333?style=for-the-badge&logo=gmail&logoColor=red" />
      </a>
      <a href="https://linkedin.com/in/pedro-sales-muniz" target="_blank">
        <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" target="_blank" />
      </a>
      <a href="https://salesp07.github.io" target="_blank">
         <img src="https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=todoist&logoColor=white" target="_blank" />
      </a>
    </td>
  </tr>
</table>


# ðŸš€Tools

<p align="left"> <a href="https://aws.amazon.com" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/aec684f0-f726-46d8-89ad-e596dbb91619" alt="aws" width="300" height="200"/> </a>  <a href="https://azure.microsoft.com/en-in/" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/4f06647f-d7d8-4d95-8cc7-9f28d109b2ee" alt="azure" width="323" height="200"/></a> <a href="https://azure.microsoft.com/en-in/" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/14069fa6-24f4-4b48-b4f5-789ae2c1ff7d" alt="azure" width="300" height="200"/></a> <a href="https://azure.microsoft.com/en-in/" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/f1fd56b5-89c3-40e5-84fb-6c2792bb2d8c" alt="azure" width="200" height="150"/></a><a href="https://azure.microsoft.com/en-in/" target="_blank" rel="noreferrer"> <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/95eb4dc6-3762-4ea2-b9f3-5218d62e6078" alt="azure" width="300" height="150"/>











---

# ðŸ¤” Explaining the project

![Copie de Management Org Chart Team Whiteboard in Black Red Green Trendy Sticker Style](https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/bf57a1ab-ae6b-45bc-9a9e-6012f87d79a3)

# ðŸŽ¥ðŸ’¬ Chat with your own Videos

<img width="943" alt="Screenshot 2023-08-30 at 19 26 45" src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/127676452/0a40dc99-9698-4ddd-94d7-d29b419b0065">


# ðŸ¦™ðŸ’¬ Llama 2 Chat with your own pdfs

This chatbot is created using the open-source Llama 2 LLM model from Meta.

Particularly, we're using the [**Llama2-7B**]
To use this app, you'll need to get your own API token.

## Other Llama 2 models to try

As mentioned above, this chatbot implementation uses the [**Llama2-7B**](https://replicate.com/a16z-infra/llama7b-v2-chat) model that was trained on 7 billion parameters.

You can also try out the larger models:
- [Llama2-13B](https://replicate.com/a16z-infra/llama13b-v2-chat)
- [Llama2-70B](https://replicate.com/replicate/llama70b-v2-chat)

## Further Reading
- [Llama 2 website](https://ai.meta.com/llama/)
- [Llama 2 technical overview](https://ai.meta.com/resources/models-and-libraries/llama/)
- [Llama 2 blog](https://ai.meta.com/blog/llama-2/)
- [Llama 2 research article](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
- [Llama 2 GitHub repo](https://github.com/facebookresearch/llama/tree/main)


---
# Media Retrieval and Captioning with CLIP

This Jupyter Notebook demonstrates how to use OpenAI's CLIP model for media retrieval and captioning. Given an input query, the code retrieves the best matching image or video frame from a provided dataset. The CLIP model is utilized to encode both text queries and media embeddings, enabling efficient similarity calculations.

## Usage

1. **Run Each Cell**: Execute each code cell in sequential order to set up the environment and define the functions.

2. **Upload Media**: Run the cells that define the Gradio interface to upload an image or video file.

3. **Enter Query**: Provide a query related to the content you're looking for.

4. **Select Mode**: Choose between "image" or "video" mode for retrieval.

5. **Provide Image Folder Path**: If using image mode, run the cells that define the image retrieval function and provide the path to the folder containing images.

6. **Results**: The interface will display the best matching media along with a caption.

## Example

<p align="center">
  <img src="https://github.com/kaoutar-lakdim/LM-Enhanced-Search/assets/74473164/50a89f82-4da1-43bc-8789-4a3bd0564f33" alt="schema">
</p>



<br/><br/>
<hr/>

<h3 align="center">
    <img src="https://readme-typing-svg.herokuapp.com/?font=Righteous&size=40&center=true&vCenter=true&width=500&height=70&duration=4000&lines=Thank+you+for+your+time+:);" />
</h3>

<br/>


---


