{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12449823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image captured_images\\image1.jpg\n",
      "Image Caption: A man with a beard and glasses looking at the camera.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import clip\n",
    "import numpy as np\n",
    "import replicate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Create a directory to store the images\n",
    "image_directory = \"captured_images\"\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "\n",
    "# Open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def capture_and_store_images():\n",
    "    image_count = 1\n",
    "    embeddings = []\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Check if frame is valid\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Display the resulting frame (optional)\n",
    "        cv2.imshow('Camera', frame)\n",
    "\n",
    "        # Preprocess the frame\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
    "        image_pil = Image.fromarray(image)\n",
    "        preprocessed_image = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        # Encode the image features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(preprocessed_image)\n",
    "\n",
    "        # Store the embeddings\n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "\n",
    "        # Save the image to the directory\n",
    "        image_path = os.path.join(image_directory, f\"image{image_count}.jpg\")\n",
    "        cv2.imwrite(image_path, frame)\n",
    "\n",
    "        # Increment the image count\n",
    "        image_count += 1\n",
    "\n",
    "        # Wait for 10 seconds\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Stop capturing images if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Convert the embeddings list to a numpy array\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Save the embeddings to a numpy file\n",
    "    np.save(\"embeddings.npy\", embeddings)\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the capture_and_store_images() function to start capturing images\n",
    "capture_and_store_images()\n",
    "\n",
    "# Load the CLIP model and its preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "text = \"a man \"  # Replace with your textual description\n",
    "\n",
    "# Preprocess the text\n",
    "text_input = clip.tokenize([text]).to(device)\n",
    "\n",
    "# Load the stored image embeddings\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "# Calculate similarity between the text and image embeddings\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_input)\n",
    "    similarities = torch.nn.functional.cosine_similarity(text_features, torch.from_numpy(embeddings).to(device))\n",
    "\n",
    "# Retrieve the index of the most similar image\n",
    "most_similar_index = torch.argmax(similarities).item()\n",
    "\n",
    "# Assuming you have a directory named \"captured_images\" containing the captured images\n",
    "image_directory = \"captured_images\"\n",
    "\n",
    "# Get the list of image filenames in the directory\n",
    "image_filenames = os.listdir(image_directory)\n",
    "\n",
    "# Check if the most_similar_index is within the range of image_filenames\n",
    "if most_similar_index < len(image_filenames):\n",
    "    # Get the path of the most similar image\n",
    "    most_similar_image_filename = os.path.join(image_directory, image_filenames[most_similar_index])\n",
    "\n",
    "    # Load and display the most similar image\n",
    "    most_similar_image = cv2.imread(most_similar_image_filename)\n",
    "    cv2.imshow(\"Most Similar Image\", most_similar_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Process the image using replicate for captioning\n",
    "    os.environ['REPLICATE_API_TOKEN'] = 'r8_8qbNrxWbMrzN2ZKDz9nG3bjI2LArp4A2pQ0xF'\n",
    "    output = replicate.run(\n",
    "        \"rmokady/clip_prefix_caption:9a34a6339872a03f45236f114321fb51fc7aa8269d38ae0ce5334969981e4cd8\",\n",
    "        input={\"image\": open(most_similar_image_filename, \"rb\")}\n",
    "    )\n",
    "\n",
    "    # Retrieve the caption from the output\n",
    "    caption = output\n",
    "    print(\"image\", most_similar_image_filename)\n",
    "    print(\"Image Caption:\", caption)\n",
    "else:\n",
    "    print(\"No image found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a073b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
